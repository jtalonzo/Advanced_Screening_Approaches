# -*- coding: utf-8 -*-
"""relation_extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WM_DGeoyT0AzjRHXGVJD-Tg1zJTcmhXI
"""

# Import libraries
import json
import itertools
import logging
import numpy as np
import torch

# Typing Imports
from typing import List, Tuple, Callable, Optional, Dict, Any, Iterable

# External Libraries
from sklearn.model_selection import train_test_split
from wasabi import Printer

# spaCy Imports
import spacy
from spacy.tokens import DocBin, Doc, Span
from spacy.scorer import PRFScore
from spacy.pipeline import TrainablePipe
from spacy.vocab import Vocab
from spacy.training import Example
from spacy.language import Language, registry

# Thinc Imports (spaCy's Deep Learning Library)
from thinc.api import Model, Linear, chain, Logistic
from thinc.types import Floats2d, Ints1d, Ragged, cast
from thinc.model import set_dropout_rate

# Set custom extension for relation annotations
if not Doc.has_extension("rel"):
    Doc.set_extension("rel", default=[])

# Load the blank English model
nlp = spacy.blank("en")
print("Blank model initialized:", nlp.pipe_names)  # Should show an empty pipeline

# Load exported Prodigy data
from google.colab import files
uploaded = files.upload()

input_file = "data_relations_utf8.jsonl"
with open(input_file, "r", encoding="utf-8") as f:
    data = [json.loads(line) for line in f]

docs = []
for example in data:
    doc = nlp.make_doc(example["text"])

    # Add entity spans
    spans = []
    for span in example.get("spans", []):
        entity = doc.char_span(span["start"], span["end"], label=span["label"])
        if entity:
            spans.append(entity)
    doc.ents = spans

    # Add relation annotations as custom data
    doc._.rel = example.get("relations", [])
    docs.append(doc)

# Split into train/dev (80/20)
train_docs, dev_docs = train_test_split(docs, test_size=0.2, random_state=42)

# Convert to spaCy's binary format
doc_bin = DocBin(attrs=["ENT_IOB", "ENT_TYPE", "HEAD", "DEP", "TAG", "LEMMA", "SENT_START"])
for doc in train_docs:
    doc_bin.add(doc)
doc_bin.to_disk("./train.spacy")

def instance_forward(
        model: Model[List[Doc], Floats2d],
        docs: List[Doc],
        is_train: bool
) -> Tuple[Floats2d, Callable]:
    tok2vec = model.get_ref("tok2vec")
    pooling = model.get_ref("pooling")
    get_instances = model.attrs["get_instances"]
    all_instances = [get_instances(doc) for doc in docs]
    tokvecs, bp_tokvecs = tok2vec(docs, is_train)

    ents = []
    lengths = []

    for doc_nr, (instances, tokvec) in enumerate(zip(all_instances, tokvecs)):
        token_indices = []
        for instance in instances:
          for ent in instance:
              token_indices.extend([i for i in range(ent.start, ent.end)])
              lengths.append(ent.end - ent.start)
        ents.append(tokvec[token_indices])
    lengths = cast(Insts1d, model.ops.asarray(lengths, dtype="int32"))
    entities = Ragged(model.ops.flatten(ents), lengths)
    pooled, bp_pooled = pooling(entities, is_train)
    relations = model.ops.reshape2f(pooled, -1, pooled.shape[1]*2)

    def backprop(d_relations: Floats2d) -> List[Doc]:
        d_pooled = model.ops.reshape2f(d_relations, d_relations.shape[0]*2, -1)
        d_ents = bp_pooled(d_pooled).datad_tokvecs = []
        ent_index = 0
        for doc_nr, instances in enumerate(all_instances):
            shape = tokvecs[doc_nr].shape
            d_tokvec = model.ops.alloc2f(*shape)
            count_occ - model.ops.alloc2f(*shape)
            for instance in instances:
                for ent in instance:
                  d_tokvec[ent.start:ent.end] += d_ents[ent_index]
                  count_occ[ent.start:ent.end] += 1
                  ent_index += ent.end - ent.start
            d_tokvec /= count_occ + 0.00000000001
            d_tokvecs.append(d_tokvecs)

        d_docs = bp_tokvecs(d_tokvecs)
        return d_docs

    return relations, backprop

def instance_init(model: Model, X: List[Doc] = None, Y: Floats2d = None) -> Model:
    tok2vec = model.get_ref("tok2vec")
    tok2vec.initialize(X)
    return model

@registry.architectures("rel_instance_tensor.v1")
def create_tensors(
    tok2vec: Model[List[Doc], List[Floats2d]],
    pooling: Model[Ragged, Floats2d],
    get_instances: Callable[[Doc], List[Tuple[Span, Span]]]
) -> Model[List[Doc], Floats2d]:
    return Model(
        "instance_tensors",
        instance_forward,
        init=instance_init,
        layers=[tok2vec, pooling],
        refs={"tok2vec": tok2vec, "pooling": pooling},
        attrs={"get_instances": get_instances},
        )

@registry.misc("rel_instance_generator.v1")
def create_instances(max_length: int) -> Callable[[Doc], List[Tuple[Span, Span]]]:
    def get_instances(doc: Doc) -> List[Tuple[Span, Span]]:
        instances = []
        for ent1 in doc.ents:
            for ent2 in doc.ents:
                if max_length and abs(ent2.start - ent1.start) <= max_length:
                    instances.append((ent1, ent2))
        return instances
    return get_instances

# Create a spaCy configuration file and save it as 'config.cfg'

config_content = """
[paths]
train = "./train.spacy"
dev = "./dev.spacy"

[nlp]
lang = "en"
pipeline = ["tok2vec", "relation_extractor"]
tokenizer = {"@tokenizers": "spacy.Tokenizer.v1"}

[components]
[components.tok2vec]
factory = "tok2vec"

[components.tok2vec.model]
@architectures = "spacy.HashEmbedCNN.v1"
depth = 2
embed_size = 2000
width = 96
window_size = 1
maxout_pieces = 3
subword_features = true
pretrained_vectors = null  # Add this field to resolve the error

[components.relation_extractor]
factory = "relation_extractor"

[components.relation_extractor.model]
@architectures = "rel_model.v1"
tok2vec = {"@ref": "components.tok2vec.model"}  # Reference tok2vec here
classification_layer = {"@architectures": "rel_classification_layer.v1", "nO": 9, "nI": 128}
instance_generator = {"@misc": "rel_instance_generator.v1"}

[training]
seed = 42
optimizer = {"@optimizers": "Adam.v1"}
patience = 10
max_epochs = 20
dropout = 0.2
accumulate_gradient = 1
gpu_allocator = "pytorch"
max_steps = 20000
eval_frequency = 200
frozen_components = []
annotating_components = []
train_corpus = "corpora.train"
dev_corpus = "corpora.dev"
batcher = {"@batchers": "spacy.batch_by_padded.v1", "size": 4096, "buffer": 256, "discard_oversize": false}

[logger]
@loggers = "spacy.ConsoleLogger.v1"
"""

# Save the content to a file named 'config.cfg'
with open("config.cfg", "w") as f:
    f.write(config_content)

print("Config file 'config.cfg' created successfully!")

import sys
from google.colab import drive
drive.mount('/content/drive')

sys.path.append("/content/drive/My Drive/NLP_Project/relation_extraction/")

import relation_extractor
print("relation_extractor loaded successfully!")

# Create a blank spaCy pipeline
nlp = spacy.blank("en")

# Add the relation_extractor component with valid configuration
nlp.add_pipe("relation_extractor", name="relation_extractor", config={
    "model": {
        "@architectures": "rel_model.v1",
        "tok2vec": {
            "@architectures": "spacy.Tok2Vec.v2",
            "embed": {
                "@architectures": "spacy.HashEmbedCNN.v1",
                "width": 128,
                "depth": 2,
                "embed_size": 2000,
                "window_size": 1,
                "maxout_pieces": 3,
                "subword_features": True,
                "pretrained_vectors": None
            },
            "encode": {
                "@architectures": "spacy.MaxoutWindowEncoder.v1",
                "width": 128,
                "depth": 2,
                "window_size": 1,
                "maxout_pieces": 3
            }
        },
        "create_instance_tensor": {
            "@architectures": "rel_instance_tensor.v1",
            "nO": 9,
            "nI": 128,
            "pooling": {"@layers": "reduce_mean.v1"},
            "tok2vec": {
                "@architectures": "spacy.Tok2Vec.v2",
                "embed": {
                    "@architectures": "spacy.HashEmbedCNN.v1",
                    "width": 128,
                    "depth": 2,
                    "embed_size": 2000,
                    "window_size": 1,
                    "maxout_pieces": 3,
                    "subword_features": True,
                    "pretrained_vectors": None
                },
                "encode": {
                    "@architectures": "spacy.MaxoutWindowEncoder.v1",
                    "width": 128,
                    "depth": 2,
                    "window_size": 1,
                    "maxout_pieces": 3
                }
            }
        },
        "instance_generator": {"@misc": "rel_instance_generator.v1", "max_length": 512},
        "classification_layer": {
            "@architectures": "rel_classification_layer.v1",
            "nO": 9,
            "nI": 128
        }
    }
})

print("relation_extractor factory registered successfully!")

# Train the model
!python -m spacy train config.cfg --output ./model --paths.train ./train.spacy --paths.dev ./dev.spacy --code "/content/drive/My Drive/NLP_Project/relation_extraction/relation_extractor.py"

# Evaluate the model
!python -m spacy evaluate ./model-best ./dev.spacy